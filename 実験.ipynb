{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"USD_JPY.csv\")\n",
    "colmuns = [\"終値\", \"始値\", \"高値\", \"安値\"] #使用するデータの選択\n",
    "processed_data = data[colmuns].iloc[::-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>日付け</th>\n",
       "      <th>終値</th>\n",
       "      <th>始値</th>\n",
       "      <th>高値</th>\n",
       "      <th>安値</th>\n",
       "      <th>出来高</th>\n",
       "      <th>変化率 %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>150.17</td>\n",
       "      <td>150.15</td>\n",
       "      <td>150.21</td>\n",
       "      <td>149.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>150.15</td>\n",
       "      <td>150.10</td>\n",
       "      <td>150.24</td>\n",
       "      <td>150.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>150.21</td>\n",
       "      <td>149.93</td>\n",
       "      <td>150.65</td>\n",
       "      <td>149.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>149.91</td>\n",
       "      <td>150.57</td>\n",
       "      <td>150.60</td>\n",
       "      <td>149.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>150.55</td>\n",
       "      <td>150.79</td>\n",
       "      <td>150.82</td>\n",
       "      <td>150.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8893</th>\n",
       "      <td>1990-01-08</td>\n",
       "      <td>144.15</td>\n",
       "      <td>144.10</td>\n",
       "      <td>144.80</td>\n",
       "      <td>143.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8894</th>\n",
       "      <td>1990-01-05</td>\n",
       "      <td>144.55</td>\n",
       "      <td>144.50</td>\n",
       "      <td>144.65</td>\n",
       "      <td>143.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8895</th>\n",
       "      <td>1990-01-04</td>\n",
       "      <td>143.37</td>\n",
       "      <td>143.33</td>\n",
       "      <td>145.94</td>\n",
       "      <td>142.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896</th>\n",
       "      <td>1990-01-03</td>\n",
       "      <td>145.37</td>\n",
       "      <td>145.32</td>\n",
       "      <td>146.90</td>\n",
       "      <td>145.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8897</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>146.60</td>\n",
       "      <td>146.55</td>\n",
       "      <td>146.78</td>\n",
       "      <td>143.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.88%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8898 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             日付け      終値      始値      高値      安値  出来高   変化率 %\n",
       "0     2024-02-19  150.17  150.15  150.21  149.88  NaN   0.01%\n",
       "1     2024-02-18  150.15  150.10  150.24  150.04  NaN  -0.04%\n",
       "2     2024-02-16  150.21  149.93  150.65  149.82  NaN   0.20%\n",
       "3     2024-02-15  149.91  150.57  150.60  149.54  NaN  -0.43%\n",
       "4     2024-02-14  150.55  150.79  150.82  150.35  NaN  -0.16%\n",
       "...          ...     ...     ...     ...     ...  ...     ...\n",
       "8893  1990-01-08  144.15  144.10  144.80  143.85  NaN  -0.28%\n",
       "8894  1990-01-05  144.55  144.50  144.65  143.00  NaN   0.82%\n",
       "8895  1990-01-04  143.37  143.33  145.94  142.80  NaN  -1.38%\n",
       "8896  1990-01-03  145.37  145.32  146.90  145.05  NaN  -0.84%\n",
       "8897  1990-01-02  146.60  146.55  146.78  143.40  NaN   1.88%\n",
       "\n",
       "[8898 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataframe, train_rate):\n",
    "    dataframe = dataframe\n",
    "    data_length = len(dataframe)\n",
    "    train_data_length = int(data_length * train_rate)\n",
    "\n",
    "    train_array = dataframe[:train_data_length]\n",
    "    test_array = dataframe[train_data_length:]\n",
    "\n",
    "    return train_array, test_array\n",
    "\n",
    "train_array, test_array = split_data(processed_data, train_rate=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocess(data_array):\n",
    "    ss = StandardScaler()\n",
    "    return ss.fit_transform(data_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array, test_array = split_data(processed_data, train_rate=0.8)\n",
    "\n",
    "preprocessed_train_data = datapreprocess(train_array)\n",
    "preprocessed_test_data = datapreprocess(test_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBatchMaxIndex(data_length, batch_size):\n",
    "    batch_max_index = 0\n",
    "    for index in reversed(range(data_length)):\n",
    "        if (index + 1)%batch_size == 0:\n",
    "            batch_max_index = index + 1\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return batch_max_index\n",
    "\n",
    "def bachifyDataset(data_array, seq_len, batch_size, num_inputs):\n",
    "\n",
    "    data_length = len(data_array)\n",
    "\n",
    "    batch_encoder_input = np.zeros((1, seq_len, 1, num_inputs))\n",
    "    batch_decoder_input = np.zeros((1, 1, 1, num_inputs))\n",
    "    batch_decoder_output = np.zeros((1, 1, 1, num_inputs))\n",
    "\n",
    "    for index in reversed(range(data_length)):\n",
    "        encoder_input_start = index - seq_len - 1\n",
    "        encoder_input_end = index - 1\n",
    "        decoder_input_index = encoder_input_end\n",
    "        decoder_output_index = decoder_input_index + 1\n",
    "        if encoder_input_start < 0:\n",
    "            break\n",
    "        else:\n",
    "            \n",
    "            encoder_input = data_array[encoder_input_start:encoder_input_end].reshape(1, seq_len, 1, num_inputs)\n",
    "            decoder_input = data_array[decoder_input_index].reshape(1, 1, 1, num_inputs)\n",
    "            decoder_output = data_array[decoder_output_index].reshape(1, 1, 1, num_inputs)\n",
    "\n",
    "            batch_encoder_input = np.concatenate([encoder_input, batch_encoder_input], axis=0)\n",
    "            batch_decoder_input = np.concatenate([decoder_input, batch_decoder_input], axis=0)\n",
    "            batch_decoder_output = np.concatenate([decoder_output, batch_decoder_output], axis=0)\n",
    "\n",
    "    batch_encoder_input = batch_encoder_input[:-1]\n",
    "    batch_decoder_input = batch_decoder_input[:-1]\n",
    "    batch_decoder_output = batch_decoder_output[:-1]\n",
    "\n",
    "    data_length = len(batch_encoder_input)\n",
    "    batch_max_index = checkBatchMaxIndex(data_length, batch_size)\n",
    "\n",
    "    return torch.tensor(batch_encoder_input[-batch_max_index:], dtype=torch.float), torch.tensor(batch_decoder_input[-batch_max_index:].reshape(-1, 1, num_inputs), dtype=torch.float), torch.tensor(batch_decoder_output[-batch_max_index:], dtype=torch.float)[:, :, :, 0].reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train_encoder_input, batch_train_decoder_input, batch_train_decoder_output = bachifyDataset(data_array=preprocessed_train_data, seq_len=64, batch_size=60, num_inputs=4)\n",
    "batch_test_encoder_input, batch_test_decoder_input, batch_test_decoder_output = bachifyDataset(data_array=preprocessed_test_data, seq_len=64, batch_size=60, num_inputs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_filters, kernel_size, pool_size):\n",
    "        super().__init__()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size, bias=True).to(device),\n",
    "            nn.ReLU().to(device),\n",
    "            nn.MaxPool2d(kernel_size=pool_size, stride=1).to(device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.reshape(-1, self.num_filters, self.num_inputs)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size, seq_len, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.input_weight = nn.Linear(input_size, hidden_size*4, bias=True).to(device)\n",
    "        self.h_weight = nn.Linear(hidden_size, hidden_size*4, bias=True).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        h_prev = torch.rand(((self.batch_size, self.hidden_size))).to(device)\n",
    "        c_prev = torch.rand(((self.batch_size, self.hidden_size))).to(device)\n",
    "        hs = torch.zeros((self.batch_size, 1, self.hidden_size)).to(device)\n",
    "        cs = torch.zeros((self.batch_size, 1, self.hidden_size)).to(device)\n",
    "        \n",
    "        for seq_idx in range(self.seq_len):\n",
    "            tmp_x = self.input_weight(x[:, seq_idx, :])\n",
    "            tmp_h = self.h_weight(h_prev)\n",
    "            \n",
    "            f = F.sigmoid(tmp_x[:, :self.hidden_size]+tmp_h[:, :self.hidden_size])\n",
    "            i = F.tanh(F.sigmoid(tmp_x[:, self.hidden_size:self.hidden_size*2]+tmp_h[:, self.hidden_size:self.hidden_size*2])) + 0.2\n",
    "            c = F.tanh(tmp_x[:, self.hidden_size*2:self.hidden_size*3]+tmp_h[:, self.hidden_size*2:self.hidden_size*3])\n",
    "            o = F.sigmoid(tmp_x[:, self.hidden_size*3:self.hidden_size*4]+tmp_h[:, self.hidden_size*3:self.hidden_size*4])\n",
    "            c_next = f*c_prev + i*c\n",
    "            h_next = o*F.tanh(c_next)\n",
    "\n",
    "            h_prev = h_next\n",
    "            c_prev = c_next\n",
    "            hs = torch.concat([hs, h_next.reshape(self.batch_size, 1, self.hidden_size)], dim=1)\n",
    "            cs = torch.concat([cs, c_next.reshape(self.batch_size, 1, self.hidden_size)], dim=1)\n",
    "        \n",
    "        hs = hs[:, 1:, :]\n",
    "        cs = cs[:, 1, :]\n",
    "        h = hs[:, 0, :].reshape(-1, 1, self.hidden_size)\n",
    "        return h, hs\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, batch_size, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.input_weight = nn.Linear(input_size, hidden_size*4, bias=True).to(device)\n",
    "        self.h_weight = nn.Linear(hidden_size, hidden_size*4, bias=True).to(device)\n",
    "        self.output_weight = nn.Linear(hidden_size*2, 1, bias=True).to(device)\n",
    "\n",
    "    def AttentionLayer(self, hs, decoder_output):\n",
    "        query = decoder_output\n",
    "        key, value = hs, hs\n",
    "        score = F.softmax(query@torch.transpose(key, 1, 2))\n",
    "        attention = score@value\n",
    "\n",
    "        return attention\n",
    "\n",
    "    def forward(self, hs, h, x):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        h_prev = h\n",
    "        c_prev = torch.rand(((self.batch_size, 1, self.hidden_size))).to(device)\n",
    "        \n",
    "        tmp_x = self.input_weight(x)\n",
    "        tmp_h = self.h_weight(h_prev)\n",
    "        f = F.sigmoid(tmp_x[:, :, :self.hidden_size]+tmp_h[:, :, :self.hidden_size])\n",
    "        i = F.tanh(F.sigmoid(tmp_x[:, :, self.hidden_size:self.hidden_size*2]+tmp_h[:, :, self.hidden_size:self.hidden_size*2])) + 0.2\n",
    "        c = F.tanh(tmp_x[:, :, self.hidden_size*2:self.hidden_size*3]+tmp_h[:, :, self.hidden_size*2:self.hidden_size*3])\n",
    "        o = F.sigmoid(tmp_x[:, :, self.hidden_size*3:self.hidden_size*4]+tmp_h[:, :, self.hidden_size*3:self.hidden_size*4])\n",
    "        c_next = f*c_prev + i*c\n",
    "        h_next = o*F.tanh(c_next)\n",
    "\n",
    "        attention = self.AttentionLayer(hs, h_next)\n",
    "\n",
    "        output = torch.concat([attention, h_next], dim=-1)\n",
    "        output = self.output_weight(output).reshape(-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "class CNN_STLSTM_AM(nn.Module):\n",
    "    def __init__(self, batch_size, seq_len, num_inputs, num_filters, kernel_size, pool_size,  input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = CNN(num_inputs=num_inputs, num_filters=num_filters, kernel_size=kernel_size, pool_size=pool_size)\n",
    "        self.encoder = Encoder(batch_size=batch_size, seq_len=seq_len, input_size=input_size, hidden_size=hidden_size)\n",
    "        self.decoder = AttentionDecoder(batch_size=batch_size, input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, batch_encoder_input, batch_decoder_input):\n",
    "        cnn_output = self.cnn(batch_encoder_input)\n",
    "        h, hs = self.encoder(cnn_output)\n",
    "        output = self.decoder(hs, h, batch_decoder_input)\n",
    "\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=60\n",
    "seq_len=64\n",
    "num_inputs=4\n",
    "num_filters=64\n",
    "kernel_size=1\n",
    "pool_size=1\n",
    "input_size=4\n",
    "hidden_size=64\n",
    "\n",
    "\n",
    "model = CNN_STLSTM_AM(batch_size, seq_len, num_inputs, num_filters, kernel_size, pool_size,  input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(dataset, batch_size):\n",
    "    mini_batches = []\n",
    "    data_length = len(dataset)\n",
    "    num_batches = data_length // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        mini_batch = dataset[start_index:end_index]\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "train_encoder_input = get_mini_batches(batch_train_encoder_input, batch_size=60)\n",
    "train_decoder_input = get_mini_batches(batch_train_decoder_input, batch_size=60)\n",
    "train_decoder_output = get_mini_batches(batch_train_decoder_output, batch_size=60)\n",
    "\n",
    "test_encoder_input = get_mini_batches(batch_test_encoder_input, batch_size=60)\n",
    "test_decoder_input = get_mini_batches(batch_test_decoder_input, batch_size=60)\n",
    "test_decoder_output = get_mini_batches(batch_test_decoder_output, batch_size=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "あなたにはmodel, batch_encoder_input, batch_decoder_input, batch_decoder_output, learning_rate, num_epochsを入力するとモデルが訓練、検証を行う関数を策してしてもらいます。ただし、この関数内でget_mini_batches、split_datasetを使用し訓練、評価用のlist型データセットを用意すること。また活性化関数はadam、学習率は0.001、損失関数はmae、エポックは50とします。また各イテレーション、エポック毎にmaeの結果をprint文で出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_and_evaluate_model(model, processed_data, learning_rate, num_epochs):\n",
    "    train_array, test_array = split_data(processed_data, train_rate=0.8)\n",
    "\n",
    "    preprocessed_train_data = datapreprocess(train_array)\n",
    "    preprocessed_test_data = datapreprocess(test_array)\n",
    "\n",
    "    batch_train_encoder_input, batch_train_decoder_input, batch_train_decoder_output = bachifyDataset(data_array=preprocessed_train_data, seq_len=64, batch_size=60, num_inputs=4)\n",
    "    batch_test_encoder_input, batch_test_decoder_input, batch_test_decoder_output = bachifyDataset(data_array=preprocessed_test_data, seq_len=64, batch_size=60, num_inputs=4)\n",
    "\n",
    "    train_encoder_input = get_mini_batches(batch_train_encoder_input, batch_size=60)\n",
    "    train_decoder_input = get_mini_batches(batch_train_decoder_input, batch_size=60)\n",
    "    train_decoder_output = get_mini_batches(batch_train_decoder_output, batch_size=60)\n",
    "\n",
    "    test_encoder_input = get_mini_batches(batch_test_encoder_input, batch_size=60)\n",
    "    test_decoder_input = get_mini_batches(batch_test_decoder_input, batch_size=60)\n",
    "    test_decoder_output = get_mini_batches(batch_test_decoder_output, batch_size=60)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i in range(len(train_encoder_input)):\n",
    "            encoder_input_batch = train_encoder_input[i].to(device)\n",
    "            decoder_input_batch = train_decoder_input[i].to(device)\n",
    "            decoder_output_batch = train_decoder_output[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(encoder_input_batch, decoder_input_batch)\n",
    "            loss = criterion(output, decoder_output_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "                  \n",
    "        train_loss /= len(train_encoder_input)\n",
    "        print(f\"[Train] Epoch:{epoch + 1}/{num_epochs}train_loss {train_loss:.4f}\")\n",
    "        model.eval()\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(test_encoder_input)):\n",
    "                encoder_input_batch = test_encoder_input[i].to(device)\n",
    "                decoder_input_batch = test_decoder_input[i].to(device)\n",
    "                decoder_output_batch = test_decoder_output[i].to(device)\n",
    "                output = model(encoder_input_batch, decoder_input_batch)\n",
    "                loss = criterion(output, decoder_output_batch)\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "            eval_loss /= len(test_encoder_input)\n",
    "            print(f\"[Eval] Epoch:{epoch + 1}/{num_epochs} eval_loss {eval_loss:.4f}\")\n",
    "            \n",
    "        print(f\" Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhei\\AppData\\Local\\Temp\\ipykernel_3692\\3519888728.py:75: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  score = F.softmax(query@torch.transpose(key, 1, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch:1/250train_loss 0.5039\n",
      "[Eval] Epoch:1/250 eval_loss 0.2236\n",
      " Epoch 1/250: Train Loss: 0.5039, Eval Loss: 0.2236\n",
      "[Train] Epoch:2/250train_loss 0.2163\n",
      "[Eval] Epoch:2/250 eval_loss 0.1975\n",
      " Epoch 2/250: Train Loss: 0.2163, Eval Loss: 0.1975\n",
      "[Train] Epoch:3/250train_loss 0.1633\n",
      "[Eval] Epoch:3/250 eval_loss 0.1895\n",
      " Epoch 3/250: Train Loss: 0.1633, Eval Loss: 0.1895\n",
      "[Train] Epoch:4/250train_loss 0.1387\n",
      "[Eval] Epoch:4/250 eval_loss 0.1121\n",
      " Epoch 4/250: Train Loss: 0.1387, Eval Loss: 0.1121\n",
      "[Train] Epoch:5/250train_loss 0.1188\n",
      "[Eval] Epoch:5/250 eval_loss 0.1506\n",
      " Epoch 5/250: Train Loss: 0.1188, Eval Loss: 0.1506\n",
      "[Train] Epoch:6/250train_loss 0.1099\n",
      "[Eval] Epoch:6/250 eval_loss 0.1202\n",
      " Epoch 6/250: Train Loss: 0.1099, Eval Loss: 0.1202\n",
      "[Train] Epoch:7/250train_loss 0.0961\n",
      "[Eval] Epoch:7/250 eval_loss 0.1416\n",
      " Epoch 7/250: Train Loss: 0.0961, Eval Loss: 0.1416\n",
      "[Train] Epoch:8/250train_loss 0.0915\n",
      "[Eval] Epoch:8/250 eval_loss 0.1184\n",
      " Epoch 8/250: Train Loss: 0.0915, Eval Loss: 0.1184\n",
      "[Train] Epoch:9/250train_loss 0.0901\n",
      "[Eval] Epoch:9/250 eval_loss 0.1337\n",
      " Epoch 9/250: Train Loss: 0.0901, Eval Loss: 0.1337\n",
      "[Train] Epoch:10/250train_loss 0.0842\n",
      "[Eval] Epoch:10/250 eval_loss 0.1119\n",
      " Epoch 10/250: Train Loss: 0.0842, Eval Loss: 0.1119\n",
      "[Train] Epoch:11/250train_loss 0.0812\n",
      "[Eval] Epoch:11/250 eval_loss 0.1101\n",
      " Epoch 11/250: Train Loss: 0.0812, Eval Loss: 0.1101\n",
      "[Train] Epoch:12/250train_loss 0.0782\n",
      "[Eval] Epoch:12/250 eval_loss 0.1078\n",
      " Epoch 12/250: Train Loss: 0.0782, Eval Loss: 0.1078\n",
      "[Train] Epoch:13/250train_loss 0.0745\n",
      "[Eval] Epoch:13/250 eval_loss 0.0986\n",
      " Epoch 13/250: Train Loss: 0.0745, Eval Loss: 0.0986\n",
      "[Train] Epoch:14/250train_loss 0.0672\n",
      "[Eval] Epoch:14/250 eval_loss 0.0953\n",
      " Epoch 14/250: Train Loss: 0.0672, Eval Loss: 0.0953\n",
      "[Train] Epoch:15/250train_loss 0.0656\n",
      "[Eval] Epoch:15/250 eval_loss 0.0928\n",
      " Epoch 15/250: Train Loss: 0.0656, Eval Loss: 0.0928\n",
      "[Train] Epoch:16/250train_loss 0.0649\n",
      "[Eval] Epoch:16/250 eval_loss 0.0919\n",
      " Epoch 16/250: Train Loss: 0.0649, Eval Loss: 0.0919\n",
      "[Train] Epoch:17/250train_loss 0.0691\n",
      "[Eval] Epoch:17/250 eval_loss 0.1004\n",
      " Epoch 17/250: Train Loss: 0.0691, Eval Loss: 0.1004\n",
      "[Train] Epoch:18/250train_loss 0.0702\n",
      "[Eval] Epoch:18/250 eval_loss 0.0912\n",
      " Epoch 18/250: Train Loss: 0.0702, Eval Loss: 0.0912\n",
      "[Train] Epoch:19/250train_loss 0.0623\n",
      "[Eval] Epoch:19/250 eval_loss 0.0987\n",
      " Epoch 19/250: Train Loss: 0.0623, Eval Loss: 0.0987\n",
      "[Train] Epoch:20/250train_loss 0.0610\n",
      "[Eval] Epoch:20/250 eval_loss 0.0770\n",
      " Epoch 20/250: Train Loss: 0.0610, Eval Loss: 0.0770\n",
      "[Train] Epoch:21/250train_loss 0.0586\n",
      "[Eval] Epoch:21/250 eval_loss 0.0920\n",
      " Epoch 21/250: Train Loss: 0.0586, Eval Loss: 0.0920\n",
      "[Train] Epoch:22/250train_loss 0.0671\n",
      "[Eval] Epoch:22/250 eval_loss 0.0910\n",
      " Epoch 22/250: Train Loss: 0.0671, Eval Loss: 0.0910\n",
      "[Train] Epoch:23/250train_loss 0.0712\n",
      "[Eval] Epoch:23/250 eval_loss 0.1018\n",
      " Epoch 23/250: Train Loss: 0.0712, Eval Loss: 0.1018\n",
      "[Train] Epoch:24/250train_loss 0.0589\n",
      "[Eval] Epoch:24/250 eval_loss 0.0861\n",
      " Epoch 24/250: Train Loss: 0.0589, Eval Loss: 0.0861\n",
      "[Train] Epoch:25/250train_loss 0.0552\n",
      "[Eval] Epoch:25/250 eval_loss 0.0678\n",
      " Epoch 25/250: Train Loss: 0.0552, Eval Loss: 0.0678\n",
      "[Train] Epoch:26/250train_loss 0.0578\n",
      "[Eval] Epoch:26/250 eval_loss 0.0863\n",
      " Epoch 26/250: Train Loss: 0.0578, Eval Loss: 0.0863\n",
      "[Train] Epoch:27/250train_loss 0.0631\n",
      "[Eval] Epoch:27/250 eval_loss 0.0884\n",
      " Epoch 27/250: Train Loss: 0.0631, Eval Loss: 0.0884\n",
      "[Train] Epoch:28/250train_loss 0.0621\n",
      "[Eval] Epoch:28/250 eval_loss 0.0988\n",
      " Epoch 28/250: Train Loss: 0.0621, Eval Loss: 0.0988\n",
      "[Train] Epoch:29/250train_loss 0.0627\n",
      "[Eval] Epoch:29/250 eval_loss 0.0792\n",
      " Epoch 29/250: Train Loss: 0.0627, Eval Loss: 0.0792\n",
      "[Train] Epoch:30/250train_loss 0.0658\n",
      "[Eval] Epoch:30/250 eval_loss 0.0775\n",
      " Epoch 30/250: Train Loss: 0.0658, Eval Loss: 0.0775\n",
      "[Train] Epoch:31/250train_loss 0.0516\n",
      "[Eval] Epoch:31/250 eval_loss 0.0698\n",
      " Epoch 31/250: Train Loss: 0.0516, Eval Loss: 0.0698\n",
      "[Train] Epoch:32/250train_loss 0.0506\n",
      "[Eval] Epoch:32/250 eval_loss 0.0591\n",
      " Epoch 32/250: Train Loss: 0.0506, Eval Loss: 0.0591\n",
      "[Train] Epoch:33/250train_loss 0.0515\n",
      "[Eval] Epoch:33/250 eval_loss 0.0655\n",
      " Epoch 33/250: Train Loss: 0.0515, Eval Loss: 0.0655\n",
      "[Train] Epoch:34/250train_loss 0.0551\n",
      "[Eval] Epoch:34/250 eval_loss 0.0706\n",
      " Epoch 34/250: Train Loss: 0.0551, Eval Loss: 0.0706\n",
      "[Train] Epoch:35/250train_loss 0.0634\n",
      "[Eval] Epoch:35/250 eval_loss 0.0957\n",
      " Epoch 35/250: Train Loss: 0.0634, Eval Loss: 0.0957\n",
      "[Train] Epoch:36/250train_loss 0.0605\n",
      "[Eval] Epoch:36/250 eval_loss 0.0936\n",
      " Epoch 36/250: Train Loss: 0.0605, Eval Loss: 0.0936\n",
      "[Train] Epoch:37/250train_loss 0.0572\n",
      "[Eval] Epoch:37/250 eval_loss 0.0748\n",
      " Epoch 37/250: Train Loss: 0.0572, Eval Loss: 0.0748\n",
      "[Train] Epoch:38/250train_loss 0.0614\n",
      "[Eval] Epoch:38/250 eval_loss 0.0655\n",
      " Epoch 38/250: Train Loss: 0.0614, Eval Loss: 0.0655\n",
      "[Train] Epoch:39/250train_loss 0.0491\n",
      "[Eval] Epoch:39/250 eval_loss 0.0944\n",
      " Epoch 39/250: Train Loss: 0.0491, Eval Loss: 0.0944\n",
      "[Train] Epoch:40/250train_loss 0.0548\n",
      "[Eval] Epoch:40/250 eval_loss 0.0730\n",
      " Epoch 40/250: Train Loss: 0.0548, Eval Loss: 0.0730\n",
      "[Train] Epoch:41/250train_loss 0.0512\n",
      "[Eval] Epoch:41/250 eval_loss 0.0761\n",
      " Epoch 41/250: Train Loss: 0.0512, Eval Loss: 0.0761\n",
      "[Train] Epoch:42/250train_loss 0.0539\n",
      "[Eval] Epoch:42/250 eval_loss 0.0708\n",
      " Epoch 42/250: Train Loss: 0.0539, Eval Loss: 0.0708\n",
      "[Train] Epoch:43/250train_loss 0.0476\n",
      "[Eval] Epoch:43/250 eval_loss 0.0673\n",
      " Epoch 43/250: Train Loss: 0.0476, Eval Loss: 0.0673\n",
      "[Train] Epoch:44/250train_loss 0.0477\n",
      "[Eval] Epoch:44/250 eval_loss 0.0526\n",
      " Epoch 44/250: Train Loss: 0.0477, Eval Loss: 0.0526\n",
      "[Train] Epoch:45/250train_loss 0.0522\n",
      "[Eval] Epoch:45/250 eval_loss 0.0648\n",
      " Epoch 45/250: Train Loss: 0.0522, Eval Loss: 0.0648\n",
      "[Train] Epoch:46/250train_loss 0.0530\n",
      "[Eval] Epoch:46/250 eval_loss 0.0623\n",
      " Epoch 46/250: Train Loss: 0.0530, Eval Loss: 0.0623\n",
      "[Train] Epoch:47/250train_loss 0.0515\n",
      "[Eval] Epoch:47/250 eval_loss 0.0562\n",
      " Epoch 47/250: Train Loss: 0.0515, Eval Loss: 0.0562\n",
      "[Train] Epoch:48/250train_loss 0.0504\n",
      "[Eval] Epoch:48/250 eval_loss 0.0487\n",
      " Epoch 48/250: Train Loss: 0.0504, Eval Loss: 0.0487\n",
      "[Train] Epoch:49/250train_loss 0.0515\n",
      "[Eval] Epoch:49/250 eval_loss 0.0586\n",
      " Epoch 49/250: Train Loss: 0.0515, Eval Loss: 0.0586\n",
      "[Train] Epoch:50/250train_loss 0.0591\n",
      "[Eval] Epoch:50/250 eval_loss 0.0670\n",
      " Epoch 50/250: Train Loss: 0.0591, Eval Loss: 0.0670\n",
      "[Train] Epoch:51/250train_loss 0.0482\n",
      "[Eval] Epoch:51/250 eval_loss 0.0838\n",
      " Epoch 51/250: Train Loss: 0.0482, Eval Loss: 0.0838\n",
      "[Train] Epoch:52/250train_loss 0.0553\n",
      "[Eval] Epoch:52/250 eval_loss 0.0588\n",
      " Epoch 52/250: Train Loss: 0.0553, Eval Loss: 0.0588\n",
      "[Train] Epoch:53/250train_loss 0.0540\n",
      "[Eval] Epoch:53/250 eval_loss 0.0844\n",
      " Epoch 53/250: Train Loss: 0.0540, Eval Loss: 0.0844\n",
      "[Train] Epoch:54/250train_loss 0.0557\n",
      "[Eval] Epoch:54/250 eval_loss 0.0749\n",
      " Epoch 54/250: Train Loss: 0.0557, Eval Loss: 0.0749\n",
      "[Train] Epoch:55/250train_loss 0.0529\n",
      "[Eval] Epoch:55/250 eval_loss 0.0779\n",
      " Epoch 55/250: Train Loss: 0.0529, Eval Loss: 0.0779\n",
      "[Train] Epoch:56/250train_loss 0.0477\n",
      "[Eval] Epoch:56/250 eval_loss 0.0572\n",
      " Epoch 56/250: Train Loss: 0.0477, Eval Loss: 0.0572\n",
      "[Train] Epoch:57/250train_loss 0.0464\n",
      "[Eval] Epoch:57/250 eval_loss 0.0695\n",
      " Epoch 57/250: Train Loss: 0.0464, Eval Loss: 0.0695\n",
      "[Train] Epoch:58/250train_loss 0.0513\n",
      "[Eval] Epoch:58/250 eval_loss 0.0618\n",
      " Epoch 58/250: Train Loss: 0.0513, Eval Loss: 0.0618\n",
      "[Train] Epoch:59/250train_loss 0.0544\n",
      "[Eval] Epoch:59/250 eval_loss 0.0579\n",
      " Epoch 59/250: Train Loss: 0.0544, Eval Loss: 0.0579\n",
      "[Train] Epoch:60/250train_loss 0.0454\n",
      "[Eval] Epoch:60/250 eval_loss 0.0803\n",
      " Epoch 60/250: Train Loss: 0.0454, Eval Loss: 0.0803\n",
      "[Train] Epoch:61/250train_loss 0.0483\n",
      "[Eval] Epoch:61/250 eval_loss 0.0616\n",
      " Epoch 61/250: Train Loss: 0.0483, Eval Loss: 0.0616\n",
      "[Train] Epoch:62/250train_loss 0.0445\n",
      "[Eval] Epoch:62/250 eval_loss 0.0501\n",
      " Epoch 62/250: Train Loss: 0.0445, Eval Loss: 0.0501\n",
      "[Train] Epoch:63/250train_loss 0.0472\n",
      "[Eval] Epoch:63/250 eval_loss 0.0608\n",
      " Epoch 63/250: Train Loss: 0.0472, Eval Loss: 0.0608\n",
      "[Train] Epoch:64/250train_loss 0.0527\n",
      "[Eval] Epoch:64/250 eval_loss 0.0637\n",
      " Epoch 64/250: Train Loss: 0.0527, Eval Loss: 0.0637\n",
      "[Train] Epoch:65/250train_loss 0.0501\n",
      "[Eval] Epoch:65/250 eval_loss 0.0544\n",
      " Epoch 65/250: Train Loss: 0.0501, Eval Loss: 0.0544\n",
      "[Train] Epoch:66/250train_loss 0.0463\n",
      "[Eval] Epoch:66/250 eval_loss 0.0466\n",
      " Epoch 66/250: Train Loss: 0.0463, Eval Loss: 0.0466\n",
      "[Train] Epoch:67/250train_loss 0.0421\n",
      "[Eval] Epoch:67/250 eval_loss 0.0501\n",
      " Epoch 67/250: Train Loss: 0.0421, Eval Loss: 0.0501\n",
      "[Train] Epoch:68/250train_loss 0.0450\n",
      "[Eval] Epoch:68/250 eval_loss 0.0454\n",
      " Epoch 68/250: Train Loss: 0.0450, Eval Loss: 0.0454\n",
      "[Train] Epoch:69/250train_loss 0.0417\n",
      "[Eval] Epoch:69/250 eval_loss 0.0482\n",
      " Epoch 69/250: Train Loss: 0.0417, Eval Loss: 0.0482\n",
      "[Train] Epoch:70/250train_loss 0.0444\n",
      "[Eval] Epoch:70/250 eval_loss 0.0537\n",
      " Epoch 70/250: Train Loss: 0.0444, Eval Loss: 0.0537\n",
      "[Train] Epoch:71/250train_loss 0.0471\n",
      "[Eval] Epoch:71/250 eval_loss 0.0551\n",
      " Epoch 71/250: Train Loss: 0.0471, Eval Loss: 0.0551\n",
      "[Train] Epoch:72/250train_loss 0.0521\n",
      "[Eval] Epoch:72/250 eval_loss 0.0685\n",
      " Epoch 72/250: Train Loss: 0.0521, Eval Loss: 0.0685\n",
      "[Train] Epoch:73/250train_loss 0.0465\n",
      "[Eval] Epoch:73/250 eval_loss 0.0403\n",
      " Epoch 73/250: Train Loss: 0.0465, Eval Loss: 0.0403\n",
      "[Train] Epoch:74/250train_loss 0.0460\n",
      "[Eval] Epoch:74/250 eval_loss 0.0526\n",
      " Epoch 74/250: Train Loss: 0.0460, Eval Loss: 0.0526\n",
      "[Train] Epoch:75/250train_loss 0.0458\n",
      "[Eval] Epoch:75/250 eval_loss 0.0501\n",
      " Epoch 75/250: Train Loss: 0.0458, Eval Loss: 0.0501\n",
      "[Train] Epoch:76/250train_loss 0.0464\n",
      "[Eval] Epoch:76/250 eval_loss 0.0503\n",
      " Epoch 76/250: Train Loss: 0.0464, Eval Loss: 0.0503\n",
      "[Train] Epoch:77/250train_loss 0.0457\n",
      "[Eval] Epoch:77/250 eval_loss 0.0509\n",
      " Epoch 77/250: Train Loss: 0.0457, Eval Loss: 0.0509\n",
      "[Train] Epoch:78/250train_loss 0.0457\n",
      "[Eval] Epoch:78/250 eval_loss 0.0469\n",
      " Epoch 78/250: Train Loss: 0.0457, Eval Loss: 0.0469\n",
      "[Train] Epoch:79/250train_loss 0.0456\n",
      "[Eval] Epoch:79/250 eval_loss 0.0533\n",
      " Epoch 79/250: Train Loss: 0.0456, Eval Loss: 0.0533\n",
      "[Train] Epoch:80/250train_loss 0.0444\n",
      "[Eval] Epoch:80/250 eval_loss 0.0506\n",
      " Epoch 80/250: Train Loss: 0.0444, Eval Loss: 0.0506\n",
      "[Train] Epoch:81/250train_loss 0.0422\n",
      "[Eval] Epoch:81/250 eval_loss 0.0419\n",
      " Epoch 81/250: Train Loss: 0.0422, Eval Loss: 0.0419\n",
      "[Train] Epoch:82/250train_loss 0.0435\n",
      "[Eval] Epoch:82/250 eval_loss 0.0458\n",
      " Epoch 82/250: Train Loss: 0.0435, Eval Loss: 0.0458\n",
      "[Train] Epoch:83/250train_loss 0.0504\n",
      "[Eval] Epoch:83/250 eval_loss 0.0582\n",
      " Epoch 83/250: Train Loss: 0.0504, Eval Loss: 0.0582\n",
      "[Train] Epoch:84/250train_loss 0.0488\n",
      "[Eval] Epoch:84/250 eval_loss 0.0584\n",
      " Epoch 84/250: Train Loss: 0.0488, Eval Loss: 0.0584\n",
      "[Train] Epoch:85/250train_loss 0.0439\n",
      "[Eval] Epoch:85/250 eval_loss 0.0387\n",
      " Epoch 85/250: Train Loss: 0.0439, Eval Loss: 0.0387\n",
      "[Train] Epoch:86/250train_loss 0.0428\n",
      "[Eval] Epoch:86/250 eval_loss 0.0488\n",
      " Epoch 86/250: Train Loss: 0.0428, Eval Loss: 0.0488\n",
      "[Train] Epoch:87/250train_loss 0.0419\n",
      "[Eval] Epoch:87/250 eval_loss 0.0420\n",
      " Epoch 87/250: Train Loss: 0.0419, Eval Loss: 0.0420\n",
      "[Train] Epoch:88/250train_loss 0.0425\n",
      "[Eval] Epoch:88/250 eval_loss 0.0450\n",
      " Epoch 88/250: Train Loss: 0.0425, Eval Loss: 0.0450\n",
      "[Train] Epoch:89/250train_loss 0.0417\n",
      "[Eval] Epoch:89/250 eval_loss 0.0470\n",
      " Epoch 89/250: Train Loss: 0.0417, Eval Loss: 0.0470\n",
      "[Train] Epoch:90/250train_loss 0.0444\n",
      "[Eval] Epoch:90/250 eval_loss 0.0513\n",
      " Epoch 90/250: Train Loss: 0.0444, Eval Loss: 0.0513\n",
      "[Train] Epoch:91/250train_loss 0.0424\n",
      "[Eval] Epoch:91/250 eval_loss 0.0420\n",
      " Epoch 91/250: Train Loss: 0.0424, Eval Loss: 0.0420\n",
      "[Train] Epoch:92/250train_loss 0.0424\n",
      "[Eval] Epoch:92/250 eval_loss 0.0431\n",
      " Epoch 92/250: Train Loss: 0.0424, Eval Loss: 0.0431\n",
      "[Train] Epoch:93/250train_loss 0.0467\n",
      "[Eval] Epoch:93/250 eval_loss 0.0537\n",
      " Epoch 93/250: Train Loss: 0.0467, Eval Loss: 0.0537\n",
      "[Train] Epoch:94/250train_loss 0.0480\n",
      "[Eval] Epoch:94/250 eval_loss 0.0549\n",
      " Epoch 94/250: Train Loss: 0.0480, Eval Loss: 0.0549\n",
      "[Train] Epoch:95/250train_loss 0.0464\n",
      "[Eval] Epoch:95/250 eval_loss 0.0494\n",
      " Epoch 95/250: Train Loss: 0.0464, Eval Loss: 0.0494\n",
      "[Train] Epoch:96/250train_loss 0.0425\n",
      "[Eval] Epoch:96/250 eval_loss 0.0414\n",
      " Epoch 96/250: Train Loss: 0.0425, Eval Loss: 0.0414\n",
      "[Train] Epoch:97/250train_loss 0.0437\n",
      "[Eval] Epoch:97/250 eval_loss 0.0564\n",
      " Epoch 97/250: Train Loss: 0.0437, Eval Loss: 0.0564\n",
      "[Train] Epoch:98/250train_loss 0.0504\n",
      "[Eval] Epoch:98/250 eval_loss 0.0568\n",
      " Epoch 98/250: Train Loss: 0.0504, Eval Loss: 0.0568\n",
      "[Train] Epoch:99/250train_loss 0.0413\n",
      "[Eval] Epoch:99/250 eval_loss 0.0482\n",
      " Epoch 99/250: Train Loss: 0.0413, Eval Loss: 0.0482\n",
      "[Train] Epoch:100/250train_loss 0.0441\n",
      "[Eval] Epoch:100/250 eval_loss 0.0521\n",
      " Epoch 100/250: Train Loss: 0.0441, Eval Loss: 0.0521\n",
      "[Train] Epoch:101/250train_loss 0.0479\n",
      "[Eval] Epoch:101/250 eval_loss 0.0520\n",
      " Epoch 101/250: Train Loss: 0.0479, Eval Loss: 0.0520\n",
      "[Train] Epoch:102/250train_loss 0.0436\n",
      "[Eval] Epoch:102/250 eval_loss 0.0418\n",
      " Epoch 102/250: Train Loss: 0.0436, Eval Loss: 0.0418\n",
      "[Train] Epoch:103/250train_loss 0.0466\n",
      "[Eval] Epoch:103/250 eval_loss 0.0518\n",
      " Epoch 103/250: Train Loss: 0.0466, Eval Loss: 0.0518\n",
      "[Train] Epoch:104/250train_loss 0.0442\n",
      "[Eval] Epoch:104/250 eval_loss 0.0408\n",
      " Epoch 104/250: Train Loss: 0.0442, Eval Loss: 0.0408\n",
      "[Train] Epoch:105/250train_loss 0.0410\n",
      "[Eval] Epoch:105/250 eval_loss 0.0420\n",
      " Epoch 105/250: Train Loss: 0.0410, Eval Loss: 0.0420\n",
      "[Train] Epoch:106/250train_loss 0.0400\n",
      "[Eval] Epoch:106/250 eval_loss 0.0421\n",
      " Epoch 106/250: Train Loss: 0.0400, Eval Loss: 0.0421\n",
      "[Train] Epoch:107/250train_loss 0.0403\n",
      "[Eval] Epoch:107/250 eval_loss 0.0413\n",
      " Epoch 107/250: Train Loss: 0.0403, Eval Loss: 0.0413\n",
      "[Train] Epoch:108/250train_loss 0.0398\n",
      "[Eval] Epoch:108/250 eval_loss 0.0371\n",
      " Epoch 108/250: Train Loss: 0.0398, Eval Loss: 0.0371\n",
      "[Train] Epoch:109/250train_loss 0.0424\n",
      "[Eval] Epoch:109/250 eval_loss 0.0502\n",
      " Epoch 109/250: Train Loss: 0.0424, Eval Loss: 0.0502\n",
      "[Train] Epoch:110/250train_loss 0.0452\n",
      "[Eval] Epoch:110/250 eval_loss 0.0537\n",
      " Epoch 110/250: Train Loss: 0.0452, Eval Loss: 0.0537\n",
      "[Train] Epoch:111/250train_loss 0.0485\n",
      "[Eval] Epoch:111/250 eval_loss 0.0598\n",
      " Epoch 111/250: Train Loss: 0.0485, Eval Loss: 0.0598\n",
      "[Train] Epoch:112/250train_loss 0.0436\n",
      "[Eval] Epoch:112/250 eval_loss 0.0363\n",
      " Epoch 112/250: Train Loss: 0.0436, Eval Loss: 0.0363\n",
      "[Train] Epoch:113/250train_loss 0.0417\n",
      "[Eval] Epoch:113/250 eval_loss 0.0468\n",
      " Epoch 113/250: Train Loss: 0.0417, Eval Loss: 0.0468\n",
      "[Train] Epoch:114/250train_loss 0.0420\n",
      "[Eval] Epoch:114/250 eval_loss 0.0490\n",
      " Epoch 114/250: Train Loss: 0.0420, Eval Loss: 0.0490\n",
      "[Train] Epoch:115/250train_loss 0.0456\n",
      "[Eval] Epoch:115/250 eval_loss 0.0498\n",
      " Epoch 115/250: Train Loss: 0.0456, Eval Loss: 0.0498\n",
      "[Train] Epoch:116/250train_loss 0.0485\n",
      "[Eval] Epoch:116/250 eval_loss 0.0488\n",
      " Epoch 116/250: Train Loss: 0.0485, Eval Loss: 0.0488\n",
      "[Train] Epoch:117/250train_loss 0.0472\n",
      "[Eval] Epoch:117/250 eval_loss 0.0470\n",
      " Epoch 117/250: Train Loss: 0.0472, Eval Loss: 0.0470\n",
      "[Train] Epoch:118/250train_loss 0.0428\n",
      "[Eval] Epoch:118/250 eval_loss 0.0456\n",
      " Epoch 118/250: Train Loss: 0.0428, Eval Loss: 0.0456\n",
      "[Train] Epoch:119/250train_loss 0.0462\n",
      "[Eval] Epoch:119/250 eval_loss 0.0487\n",
      " Epoch 119/250: Train Loss: 0.0462, Eval Loss: 0.0487\n",
      "[Train] Epoch:120/250train_loss 0.0426\n",
      "[Eval] Epoch:120/250 eval_loss 0.0405\n",
      " Epoch 120/250: Train Loss: 0.0426, Eval Loss: 0.0405\n",
      "[Train] Epoch:121/250train_loss 0.0414\n",
      "[Eval] Epoch:121/250 eval_loss 0.0438\n",
      " Epoch 121/250: Train Loss: 0.0414, Eval Loss: 0.0438\n",
      "[Train] Epoch:122/250train_loss 0.0425\n",
      "[Eval] Epoch:122/250 eval_loss 0.0459\n",
      " Epoch 122/250: Train Loss: 0.0425, Eval Loss: 0.0459\n",
      "[Train] Epoch:123/250train_loss 0.0434\n",
      "[Eval] Epoch:123/250 eval_loss 0.0421\n",
      " Epoch 123/250: Train Loss: 0.0434, Eval Loss: 0.0421\n",
      "[Train] Epoch:124/250train_loss 0.0436\n",
      "[Eval] Epoch:124/250 eval_loss 0.0465\n",
      " Epoch 124/250: Train Loss: 0.0436, Eval Loss: 0.0465\n",
      "[Train] Epoch:125/250train_loss 0.0439\n",
      "[Eval] Epoch:125/250 eval_loss 0.0450\n",
      " Epoch 125/250: Train Loss: 0.0439, Eval Loss: 0.0450\n",
      "[Train] Epoch:126/250train_loss 0.0409\n",
      "[Eval] Epoch:126/250 eval_loss 0.0419\n",
      " Epoch 126/250: Train Loss: 0.0409, Eval Loss: 0.0419\n",
      "[Train] Epoch:127/250train_loss 0.0401\n",
      "[Eval] Epoch:127/250 eval_loss 0.0412\n",
      " Epoch 127/250: Train Loss: 0.0401, Eval Loss: 0.0412\n",
      "[Train] Epoch:128/250train_loss 0.0411\n",
      "[Eval] Epoch:128/250 eval_loss 0.0442\n",
      " Epoch 128/250: Train Loss: 0.0411, Eval Loss: 0.0442\n",
      "[Train] Epoch:129/250train_loss 0.0443\n",
      "[Eval] Epoch:129/250 eval_loss 0.0475\n",
      " Epoch 129/250: Train Loss: 0.0443, Eval Loss: 0.0475\n",
      "[Train] Epoch:130/250train_loss 0.0431\n",
      "[Eval] Epoch:130/250 eval_loss 0.0439\n",
      " Epoch 130/250: Train Loss: 0.0431, Eval Loss: 0.0439\n",
      "[Train] Epoch:131/250train_loss 0.0433\n",
      "[Eval] Epoch:131/250 eval_loss 0.0455\n",
      " Epoch 131/250: Train Loss: 0.0433, Eval Loss: 0.0455\n",
      "[Train] Epoch:132/250train_loss 0.0458\n",
      "[Eval] Epoch:132/250 eval_loss 0.0476\n",
      " Epoch 132/250: Train Loss: 0.0458, Eval Loss: 0.0476\n",
      "[Train] Epoch:133/250train_loss 0.0432\n",
      "[Eval] Epoch:133/250 eval_loss 0.0452\n",
      " Epoch 133/250: Train Loss: 0.0432, Eval Loss: 0.0452\n",
      "[Train] Epoch:134/250train_loss 0.0408\n",
      "[Eval] Epoch:134/250 eval_loss 0.0410\n",
      " Epoch 134/250: Train Loss: 0.0408, Eval Loss: 0.0410\n",
      "[Train] Epoch:135/250train_loss 0.0412\n",
      "[Eval] Epoch:135/250 eval_loss 0.0432\n",
      " Epoch 135/250: Train Loss: 0.0412, Eval Loss: 0.0432\n",
      "[Train] Epoch:136/250train_loss 0.0429\n",
      "[Eval] Epoch:136/250 eval_loss 0.0436\n",
      " Epoch 136/250: Train Loss: 0.0429, Eval Loss: 0.0436\n",
      "[Train] Epoch:137/250train_loss 0.0449\n",
      "[Eval] Epoch:137/250 eval_loss 0.0470\n",
      " Epoch 137/250: Train Loss: 0.0449, Eval Loss: 0.0470\n",
      "[Train] Epoch:138/250train_loss 0.0419\n",
      "[Eval] Epoch:138/250 eval_loss 0.0418\n",
      " Epoch 138/250: Train Loss: 0.0419, Eval Loss: 0.0418\n",
      "[Train] Epoch:139/250train_loss 0.0414\n",
      "[Eval] Epoch:139/250 eval_loss 0.0439\n",
      " Epoch 139/250: Train Loss: 0.0414, Eval Loss: 0.0439\n",
      "[Train] Epoch:140/250train_loss 0.0462\n",
      "[Eval] Epoch:140/250 eval_loss 0.0469\n",
      " Epoch 140/250: Train Loss: 0.0462, Eval Loss: 0.0469\n",
      "[Train] Epoch:141/250train_loss 0.0389\n",
      "[Eval] Epoch:141/250 eval_loss 0.0424\n",
      " Epoch 141/250: Train Loss: 0.0389, Eval Loss: 0.0424\n",
      "[Train] Epoch:142/250train_loss 0.0427\n",
      "[Eval] Epoch:142/250 eval_loss 0.0481\n",
      " Epoch 142/250: Train Loss: 0.0427, Eval Loss: 0.0481\n",
      "[Train] Epoch:143/250train_loss 0.0433\n",
      "[Eval] Epoch:143/250 eval_loss 0.0428\n",
      " Epoch 143/250: Train Loss: 0.0433, Eval Loss: 0.0428\n",
      "[Train] Epoch:144/250train_loss 0.0426\n",
      "[Eval] Epoch:144/250 eval_loss 0.0418\n",
      " Epoch 144/250: Train Loss: 0.0426, Eval Loss: 0.0418\n",
      "[Train] Epoch:145/250train_loss 0.0415\n",
      "[Eval] Epoch:145/250 eval_loss 0.0392\n",
      " Epoch 145/250: Train Loss: 0.0415, Eval Loss: 0.0392\n",
      "[Train] Epoch:146/250train_loss 0.0425\n",
      "[Eval] Epoch:146/250 eval_loss 0.0411\n",
      " Epoch 146/250: Train Loss: 0.0425, Eval Loss: 0.0411\n",
      "[Train] Epoch:147/250train_loss 0.0429\n",
      "[Eval] Epoch:147/250 eval_loss 0.0420\n",
      " Epoch 147/250: Train Loss: 0.0429, Eval Loss: 0.0420\n",
      "[Train] Epoch:148/250train_loss 0.0434\n",
      "[Eval] Epoch:148/250 eval_loss 0.0463\n",
      " Epoch 148/250: Train Loss: 0.0434, Eval Loss: 0.0463\n",
      "[Train] Epoch:149/250train_loss 0.0421\n",
      "[Eval] Epoch:149/250 eval_loss 0.0400\n",
      " Epoch 149/250: Train Loss: 0.0421, Eval Loss: 0.0400\n",
      "[Train] Epoch:150/250train_loss 0.0413\n",
      "[Eval] Epoch:150/250 eval_loss 0.0432\n",
      " Epoch 150/250: Train Loss: 0.0413, Eval Loss: 0.0432\n",
      "[Train] Epoch:151/250train_loss 0.0440\n",
      "[Eval] Epoch:151/250 eval_loss 0.0431\n",
      " Epoch 151/250: Train Loss: 0.0440, Eval Loss: 0.0431\n",
      "[Train] Epoch:152/250train_loss 0.0420\n",
      "[Eval] Epoch:152/250 eval_loss 0.0420\n",
      " Epoch 152/250: Train Loss: 0.0420, Eval Loss: 0.0420\n",
      "[Train] Epoch:153/250train_loss 0.0404\n",
      "[Eval] Epoch:153/250 eval_loss 0.0448\n",
      " Epoch 153/250: Train Loss: 0.0404, Eval Loss: 0.0448\n",
      "[Train] Epoch:154/250train_loss 0.0453\n",
      "[Eval] Epoch:154/250 eval_loss 0.0427\n",
      " Epoch 154/250: Train Loss: 0.0453, Eval Loss: 0.0427\n",
      "[Train] Epoch:155/250train_loss 0.0416\n",
      "[Eval] Epoch:155/250 eval_loss 0.0415\n",
      " Epoch 155/250: Train Loss: 0.0416, Eval Loss: 0.0415\n",
      "[Train] Epoch:156/250train_loss 0.0408\n",
      "[Eval] Epoch:156/250 eval_loss 0.0388\n",
      " Epoch 156/250: Train Loss: 0.0408, Eval Loss: 0.0388\n",
      "[Train] Epoch:157/250train_loss 0.0430\n",
      "[Eval] Epoch:157/250 eval_loss 0.0437\n",
      " Epoch 157/250: Train Loss: 0.0430, Eval Loss: 0.0437\n",
      "[Train] Epoch:158/250train_loss 0.0435\n",
      "[Eval] Epoch:158/250 eval_loss 0.0427\n",
      " Epoch 158/250: Train Loss: 0.0435, Eval Loss: 0.0427\n",
      "[Train] Epoch:159/250train_loss 0.0384\n",
      "[Eval] Epoch:159/250 eval_loss 0.0375\n",
      " Epoch 159/250: Train Loss: 0.0384, Eval Loss: 0.0375\n",
      "[Train] Epoch:160/250train_loss 0.0401\n",
      "[Eval] Epoch:160/250 eval_loss 0.0442\n",
      " Epoch 160/250: Train Loss: 0.0401, Eval Loss: 0.0442\n",
      "[Train] Epoch:161/250train_loss 0.0418\n",
      "[Eval] Epoch:161/250 eval_loss 0.0388\n",
      " Epoch 161/250: Train Loss: 0.0418, Eval Loss: 0.0388\n",
      "[Train] Epoch:162/250train_loss 0.0415\n",
      "[Eval] Epoch:162/250 eval_loss 0.0416\n",
      " Epoch 162/250: Train Loss: 0.0415, Eval Loss: 0.0416\n",
      "[Train] Epoch:163/250train_loss 0.0429\n",
      "[Eval] Epoch:163/250 eval_loss 0.0426\n",
      " Epoch 163/250: Train Loss: 0.0429, Eval Loss: 0.0426\n",
      "[Train] Epoch:164/250train_loss 0.0420\n",
      "[Eval] Epoch:164/250 eval_loss 0.0429\n",
      " Epoch 164/250: Train Loss: 0.0420, Eval Loss: 0.0429\n",
      "[Train] Epoch:165/250train_loss 0.0437\n",
      "[Eval] Epoch:165/250 eval_loss 0.0407\n",
      " Epoch 165/250: Train Loss: 0.0437, Eval Loss: 0.0407\n",
      "[Train] Epoch:166/250train_loss 0.0415\n",
      "[Eval] Epoch:166/250 eval_loss 0.0394\n",
      " Epoch 166/250: Train Loss: 0.0415, Eval Loss: 0.0394\n",
      "[Train] Epoch:167/250train_loss 0.0420\n",
      "[Eval] Epoch:167/250 eval_loss 0.0422\n",
      " Epoch 167/250: Train Loss: 0.0420, Eval Loss: 0.0422\n",
      "[Train] Epoch:168/250train_loss 0.0423\n",
      "[Eval] Epoch:168/250 eval_loss 0.0413\n",
      " Epoch 168/250: Train Loss: 0.0423, Eval Loss: 0.0413\n",
      "[Train] Epoch:169/250train_loss 0.0420\n",
      "[Eval] Epoch:169/250 eval_loss 0.0405\n",
      " Epoch 169/250: Train Loss: 0.0420, Eval Loss: 0.0405\n",
      "[Train] Epoch:170/250train_loss 0.0409\n",
      "[Eval] Epoch:170/250 eval_loss 0.0413\n",
      " Epoch 170/250: Train Loss: 0.0409, Eval Loss: 0.0413\n",
      "[Train] Epoch:171/250train_loss 0.0408\n",
      "[Eval] Epoch:171/250 eval_loss 0.0411\n",
      " Epoch 171/250: Train Loss: 0.0408, Eval Loss: 0.0411\n",
      "[Train] Epoch:172/250train_loss 0.0401\n",
      "[Eval] Epoch:172/250 eval_loss 0.0396\n",
      " Epoch 172/250: Train Loss: 0.0401, Eval Loss: 0.0396\n",
      "[Train] Epoch:173/250train_loss 0.0395\n",
      "[Eval] Epoch:173/250 eval_loss 0.0399\n",
      " Epoch 173/250: Train Loss: 0.0395, Eval Loss: 0.0399\n",
      "[Train] Epoch:174/250train_loss 0.0431\n",
      "[Eval] Epoch:174/250 eval_loss 0.0443\n",
      " Epoch 174/250: Train Loss: 0.0431, Eval Loss: 0.0443\n",
      "[Train] Epoch:175/250train_loss 0.0414\n",
      "[Eval] Epoch:175/250 eval_loss 0.0392\n",
      " Epoch 175/250: Train Loss: 0.0414, Eval Loss: 0.0392\n",
      "[Train] Epoch:176/250train_loss 0.0387\n",
      "[Eval] Epoch:176/250 eval_loss 0.0379\n",
      " Epoch 176/250: Train Loss: 0.0387, Eval Loss: 0.0379\n",
      "[Train] Epoch:177/250train_loss 0.0391\n",
      "[Eval] Epoch:177/250 eval_loss 0.0410\n",
      " Epoch 177/250: Train Loss: 0.0391, Eval Loss: 0.0410\n",
      "[Train] Epoch:178/250train_loss 0.0442\n",
      "[Eval] Epoch:178/250 eval_loss 0.0429\n",
      " Epoch 178/250: Train Loss: 0.0442, Eval Loss: 0.0429\n",
      "[Train] Epoch:179/250train_loss 0.0416\n",
      "[Eval] Epoch:179/250 eval_loss 0.0412\n",
      " Epoch 179/250: Train Loss: 0.0416, Eval Loss: 0.0412\n",
      "[Train] Epoch:180/250train_loss 0.0418\n",
      "[Eval] Epoch:180/250 eval_loss 0.0401\n",
      " Epoch 180/250: Train Loss: 0.0418, Eval Loss: 0.0401\n",
      "[Train] Epoch:181/250train_loss 0.0431\n",
      "[Eval] Epoch:181/250 eval_loss 0.0434\n",
      " Epoch 181/250: Train Loss: 0.0431, Eval Loss: 0.0434\n",
      "[Train] Epoch:182/250train_loss 0.0428\n",
      "[Eval] Epoch:182/250 eval_loss 0.0401\n",
      " Epoch 182/250: Train Loss: 0.0428, Eval Loss: 0.0401\n",
      "[Train] Epoch:183/250train_loss 0.0401\n",
      "[Eval] Epoch:183/250 eval_loss 0.0399\n",
      " Epoch 183/250: Train Loss: 0.0401, Eval Loss: 0.0399\n",
      "[Train] Epoch:184/250train_loss 0.0409\n",
      "[Eval] Epoch:184/250 eval_loss 0.0407\n",
      " Epoch 184/250: Train Loss: 0.0409, Eval Loss: 0.0407\n",
      "[Train] Epoch:185/250train_loss 0.0417\n",
      "[Eval] Epoch:185/250 eval_loss 0.0419\n",
      " Epoch 185/250: Train Loss: 0.0417, Eval Loss: 0.0419\n",
      "[Train] Epoch:186/250train_loss 0.0413\n",
      "[Eval] Epoch:186/250 eval_loss 0.0395\n",
      " Epoch 186/250: Train Loss: 0.0413, Eval Loss: 0.0395\n",
      "[Train] Epoch:187/250train_loss 0.0425\n",
      "[Eval] Epoch:187/250 eval_loss 0.0423\n",
      " Epoch 187/250: Train Loss: 0.0425, Eval Loss: 0.0423\n",
      "[Train] Epoch:188/250train_loss 0.0382\n",
      "[Eval] Epoch:188/250 eval_loss 0.0372\n",
      " Epoch 188/250: Train Loss: 0.0382, Eval Loss: 0.0372\n",
      "[Train] Epoch:189/250train_loss 0.0436\n",
      "[Eval] Epoch:189/250 eval_loss 0.0475\n",
      " Epoch 189/250: Train Loss: 0.0436, Eval Loss: 0.0475\n",
      "[Train] Epoch:190/250train_loss 0.0422\n",
      "[Eval] Epoch:190/250 eval_loss 0.0425\n",
      " Epoch 190/250: Train Loss: 0.0422, Eval Loss: 0.0425\n",
      "[Train] Epoch:191/250train_loss 0.0412\n",
      "[Eval] Epoch:191/250 eval_loss 0.0405\n",
      " Epoch 191/250: Train Loss: 0.0412, Eval Loss: 0.0405\n",
      "[Train] Epoch:192/250train_loss 0.0399\n",
      "[Eval] Epoch:192/250 eval_loss 0.0388\n",
      " Epoch 192/250: Train Loss: 0.0399, Eval Loss: 0.0388\n",
      "[Train] Epoch:193/250train_loss 0.0394\n",
      "[Eval] Epoch:193/250 eval_loss 0.0386\n",
      " Epoch 193/250: Train Loss: 0.0394, Eval Loss: 0.0386\n",
      "[Train] Epoch:194/250train_loss 0.0414\n",
      "[Eval] Epoch:194/250 eval_loss 0.0426\n",
      " Epoch 194/250: Train Loss: 0.0414, Eval Loss: 0.0426\n",
      "[Train] Epoch:195/250train_loss 0.0446\n",
      "[Eval] Epoch:195/250 eval_loss 0.0399\n",
      " Epoch 195/250: Train Loss: 0.0446, Eval Loss: 0.0399\n",
      "[Train] Epoch:196/250train_loss 0.0386\n",
      "[Eval] Epoch:196/250 eval_loss 0.0393\n",
      " Epoch 196/250: Train Loss: 0.0386, Eval Loss: 0.0393\n",
      "[Train] Epoch:197/250train_loss 0.0414\n",
      "[Eval] Epoch:197/250 eval_loss 0.0422\n",
      " Epoch 197/250: Train Loss: 0.0414, Eval Loss: 0.0422\n",
      "[Train] Epoch:198/250train_loss 0.0427\n",
      "[Eval] Epoch:198/250 eval_loss 0.0383\n",
      " Epoch 198/250: Train Loss: 0.0427, Eval Loss: 0.0383\n",
      "[Train] Epoch:199/250train_loss 0.0408\n",
      "[Eval] Epoch:199/250 eval_loss 0.0397\n",
      " Epoch 199/250: Train Loss: 0.0408, Eval Loss: 0.0397\n",
      "[Train] Epoch:200/250train_loss 0.0421\n",
      "[Eval] Epoch:200/250 eval_loss 0.0408\n",
      " Epoch 200/250: Train Loss: 0.0421, Eval Loss: 0.0408\n",
      "[Train] Epoch:201/250train_loss 0.0398\n",
      "[Eval] Epoch:201/250 eval_loss 0.0383\n",
      " Epoch 201/250: Train Loss: 0.0398, Eval Loss: 0.0383\n",
      "[Train] Epoch:202/250train_loss 0.0404\n",
      "[Eval] Epoch:202/250 eval_loss 0.0415\n",
      " Epoch 202/250: Train Loss: 0.0404, Eval Loss: 0.0415\n",
      "[Train] Epoch:203/250train_loss 0.0427\n",
      "[Eval] Epoch:203/250 eval_loss 0.0422\n",
      " Epoch 203/250: Train Loss: 0.0427, Eval Loss: 0.0422\n",
      "[Train] Epoch:204/250train_loss 0.0402\n",
      "[Eval] Epoch:204/250 eval_loss 0.0394\n",
      " Epoch 204/250: Train Loss: 0.0402, Eval Loss: 0.0394\n",
      "[Train] Epoch:205/250train_loss 0.0417\n",
      "[Eval] Epoch:205/250 eval_loss 0.0398\n",
      " Epoch 205/250: Train Loss: 0.0417, Eval Loss: 0.0398\n",
      "[Train] Epoch:206/250train_loss 0.0408\n",
      "[Eval] Epoch:206/250 eval_loss 0.0413\n",
      " Epoch 206/250: Train Loss: 0.0408, Eval Loss: 0.0413\n",
      "[Train] Epoch:207/250train_loss 0.0396\n",
      "[Eval] Epoch:207/250 eval_loss 0.0397\n",
      " Epoch 207/250: Train Loss: 0.0396, Eval Loss: 0.0397\n",
      "[Train] Epoch:208/250train_loss 0.0427\n",
      "[Eval] Epoch:208/250 eval_loss 0.0415\n",
      " Epoch 208/250: Train Loss: 0.0427, Eval Loss: 0.0415\n",
      "[Train] Epoch:209/250train_loss 0.0407\n",
      "[Eval] Epoch:209/250 eval_loss 0.0374\n",
      " Epoch 209/250: Train Loss: 0.0407, Eval Loss: 0.0374\n",
      "[Train] Epoch:210/250train_loss 0.0383\n",
      "[Eval] Epoch:210/250 eval_loss 0.0368\n",
      " Epoch 210/250: Train Loss: 0.0383, Eval Loss: 0.0368\n",
      "[Train] Epoch:211/250train_loss 0.0393\n",
      "[Eval] Epoch:211/250 eval_loss 0.0410\n",
      " Epoch 211/250: Train Loss: 0.0393, Eval Loss: 0.0410\n",
      "[Train] Epoch:212/250train_loss 0.0399\n",
      "[Eval] Epoch:212/250 eval_loss 0.0373\n",
      " Epoch 212/250: Train Loss: 0.0399, Eval Loss: 0.0373\n",
      "[Train] Epoch:213/250train_loss 0.0412\n",
      "[Eval] Epoch:213/250 eval_loss 0.0414\n",
      " Epoch 213/250: Train Loss: 0.0412, Eval Loss: 0.0414\n",
      "[Train] Epoch:214/250train_loss 0.0415\n",
      "[Eval] Epoch:214/250 eval_loss 0.0395\n",
      " Epoch 214/250: Train Loss: 0.0415, Eval Loss: 0.0395\n",
      "[Train] Epoch:215/250train_loss 0.0409\n",
      "[Eval] Epoch:215/250 eval_loss 0.0401\n",
      " Epoch 215/250: Train Loss: 0.0409, Eval Loss: 0.0401\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(model, processed_data, learning_rate=0.001, num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
